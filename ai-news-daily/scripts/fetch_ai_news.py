#!/usr/bin/env python3
"""
AI æ¯æ—¥æ–°é—» - ä»ä¸“ä¸š AI åª’ä½“è·å–æ–°é—»
ç‰¹ç‚¹ï¼šè‡ªåŠ¨è¯†åˆ«å…¬å¸å’Œæœºæ„ï¼Œç®€æ´ç¾è§‚çš„è¾“å‡º
"""

import argparse
import json
import os
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set
from urllib.parse import urljoin
import time

# å°è¯•å¯¼å…¥å¯é€‰ä¾èµ–
try:
    import feedparser
    FEEDPARSER_AVAILABLE = True
except ImportError:
    FEEDPARSER_AVAILABLE = False

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# ç¿»è¯‘æ”¯æŒ
try:
    import translators as ts
    TRANSLATOR_AVAILABLE = True
except ImportError:
    TRANSLATOR_AVAILABLE = False


# ============================================
# ä¸“ä¸š AI æ–°é—»åª’ä½“é…ç½®
# ============================================
SOURCES = {
    # --- è‹±æ–‡ç»¼åˆ AI åª’ä½“ ---
    "marktechpost": {
        "name": "MarkTechPost",
        "url": "https://www.marktechpost.com/feed/",
        "type": "rss",
        "category": "business",
        "language": "en",
    },
    "mit-tech-review": {
        "name": "MIT Technology Review",
        "url": "https://www.technologyreview.com/topic/artificial-intelligence/rss/",
        "type": "rss",
        "category": "research",
        "language": "en",
    },
    "venturebeat-ai": {
        "name": "VentureBeat AI",
        "url": "https://venturebeat.com/category/ai/feed/",
        "type": "rss",
        "category": "business",
        "language": "en",
    },
    "synced-review": {
        "name": "Synced Review",
        "url": "https://syncedreview.com/feed/",
        "type": "rss",
        "category": "business",
        "language": "en",
    },
    "ai-news": {
        "name": "AI News",
        "url": "https://www.artificialintelligence-news.com/feed/",
        "type": "rss",
        "category": "business",
        "language": "en",
    },
    "machinelearningmastery": {
        "name": "Machine Learning Mastery",
        "url": "https://machinelearningmastery.com/blog/feed/",
        "type": "rss",
        "category": "research",
        "language": "en",
    },
    
    # --- ä¸­æ–‡ AI åª’ä½“ ---
    "jiqizhixin": {
        "name": "æœºå™¨ä¹‹å¿ƒ",
        "url": "https://www.jiqizhixin.com/rss",
        "type": "rss",
        "category": "business",
        "language": "zh",
    },
    "qbitai": {
        "name": "é‡å­ä½",
        "url": "https://www.qbitai.com/feed",
        "type": "rss",
        "category": "business",
        "language": "zh",
    },
    
    # --- å®˜æ–¹åšå®¢ï¼ˆå¯é€‰ï¼‰---
    "openai": {
        "name": "OpenAI åšå®¢", 
        "url": "https://openai.com/blog/rss.xml",
        "type": "rss",
        "category": "releases",
        "language": "en"
    },
    "anthropic": {
        "name": "Anthropic åšå®¢",
        "url": "https://www.anthropic.com/blog/rss.xml",
        "type": "rss",
        "category": "research",
        "language": "en"
    },
    "deepmind": {
        "name": "Google DeepMind",
        "url": "https://deepmind.google/blog/rss.xml",
        "type": "rss",
        "category": "research",
        "language": "en"
    },
    "meta-ai": {
        "name": "Meta AI åšå®¢",
        "url": "https://ai.meta.com/blog/rss/",
        "type": "rss",
        "category": "research",
        "language": "en"
    },
    
    # --- å­¦æœ¯ä¸ç ”ç©¶ ---
    "arxiv-ai": {
        "name": "arXiv cs.AI",
        "url": "http://export.arxiv.org/rss/cs.AI",
        "type": "rss",
        "category": "research",
        "language": "en"
    },
    "paperswithcode": {
        "name": "Papers with Code",
        "url": "https://paperswithcode.com/rss",
        "type": "rss",
        "category": "research",
        "language": "en"
    },
}

# ============================================
# å…¬å¸å’Œæœºæ„è¯†åˆ«é…ç½®
# ============================================
COMPANY_KEYWORDS = {
    # å›½å¤–å¤§å‚
    "OpenAI": ["openai", "gpt", "chatgpt", "dall-e", "sora", "o1", "o3"],
    "Google": ["google", "deepmind", "gemini", "bard", "alphago", "alphafold", "waymo"],
    "Anthropic": ["anthropic", "claude"],
    "Meta": ["meta", "facebook", "llama", "pytorch"],
    "Microsoft": ["microsoft", "azure", "copilot", "bing"],
    "NVIDIA": ["nvidia", "geforce", "rtx", "cuda", "hopper", "blackwell"],
    "Amazon": ["amazon", "aws", "alexa"],
    "Apple": ["apple", "siri"],
    "Tesla": ["tesla", "optimus"],
    "Stability AI": ["stability ai", "stable diffusion"],
    "Midjourney": ["midjourney"],
    "Hugging Face": ["huggingface", "hugging face", "transformers"],
    "Cohere": ["cohere"],
    "Perplexity": ["perplexity"],
    "Midjourney": ["midjourney"],
    
    # å›½å†…å¤§å‚
    "é˜¿é‡Œå·´å·´": ["é˜¿é‡Œ", "alibaba", "é€šä¹‰åƒé—®", "qwen", "è¾¾æ‘©é™¢"],
    "å­—èŠ‚è·³åŠ¨": ["å­—èŠ‚", "bytedance", "è±†åŒ…", "äº‘é›€", "doubao"],
    "ç™¾åº¦": ["ç™¾åº¦", "baidu", "æ–‡å¿ƒä¸€è¨€", "ernie", "apollo", "é£æ¡¨"],
    "è…¾è®¯": ["è…¾è®¯", "tencent", "æ··å…ƒ", "hunyuan"],
    "åä¸º": ["åä¸º", "huawei", "ç›˜å¤", "mindspore", "æ˜‡è…¾"],
    "æ™ºè°± AI": ["æ™ºè°±", "chatglm", "glm", "zhipu"],
    "æœˆä¹‹æš—é¢": ["æœˆä¹‹æš—é¢", "kimi"],
    "MiniMax": ["minimax", "abab"],
    "é›¶ä¸€ä¸‡ç‰©": ["é›¶ä¸€ä¸‡ç‰©", "01.ai", "yi"],
    "ç™¾å·æ™ºèƒ½": ["ç™¾å·", "baichuan"],
    "å•†æ±¤": ["å•†æ±¤", "sensetime", "ä¹¦ç”Ÿ"],
    "ç§‘å¤§è®¯é£": ["è®¯é£", "iflytek", "æ˜Ÿç«"],
    "ç†æƒ³æ±½è½¦": ["ç†æƒ³", "li auto"],
    "å°é¹": ["å°é¹", "xpeng"],
    "è”šæ¥": ["è”šæ¥", "nio"],
    "å°ç±³": ["å°ç±³", "xiaomi"],
    
    # ç ”ç©¶æœºæ„
    "MIT": ["mit", "éº»çœç†å·¥"],
    "Stanford": ["stanford", "æ–¯å¦ç¦"],
    "Berkeley": ["berkeley", "ä¼¯å…‹åˆ©"],
    "CMU": ["cmu", "å¡å†…åŸºæ¢…éš†"],
    "æ¸…å": ["æ¸…å", "tsinghua"],
    "åŒ—å¤§": ["åŒ—å¤§", "peking university"],
    "ä¸­ç§‘é™¢": ["ä¸­ç§‘é™¢", "cas"],
    "UIUC": ["uiuc", "ä¼Šåˆ©è¯ºä¼Š"],
}

# åˆ†ç±»å›¾æ ‡
CATEGORY_ICONS = {
    "releases": "ğŸš€",
    "research": "ğŸ”¬",
    "business": "ğŸ’°",
    "products": "ğŸ“±",
    "community": "ğŸ’¬",
    "general": "ğŸ“°"
}


def detect_companies(text: str) -> List[str]:
    """è¯†åˆ«æ–‡æœ¬ä¸­æåˆ°çš„å…¬å¸å’Œæœºæ„"""
    if not text:
        return []
    
    text_lower = text.lower()
    found_companies = []
    
    for company, keywords in COMPANY_KEYWORDS.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                found_companies.append(company)
                break
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    result = []
    for c in found_companies:
        if c not in seen:
            seen.add(c)
            result.append(c)
    
    return result[:5]  # æœ€å¤šè¿”å›5ä¸ª


def parse_date(date_str: str) -> Optional[datetime]:
    """è§£æå„ç§æ—¥æœŸæ ¼å¼"""
    if not date_str or date_str == "æœªçŸ¥":
        return None
    
    date_formats = [
        "%a, %d %b %Y %H:%M:%S %z",
        "%a, %d %b %Y %H:%M:%S %Z",
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%dT%H:%M:%S.%f%z",
        "%Y-%m-%d %H:%M:%S",
        "%d %b %Y %H:%M:%S %z",
        "%B %d, %Y",
        "%b %d, %Y",
        "%Y-%m-%d",
    ]
    
    date_str = date_str.strip()
    
    for fmt in date_formats:
        try:
            return datetime.strptime(date_str, fmt)
        except ValueError:
            continue
    
    return None


class NewsTranslator:
    """ç¿»è¯‘æ–°é—»å†…å®¹"""
    
    def __init__(self, translator_engine: str = "bing"):
        self.translator_engine = translator_engine
        self._cache: Dict[str, str] = {}
        
    def translate(self, text: str) -> str:
        if not text or not TRANSLATOR_AVAILABLE:
            return text
            
        if self._is_mostly_chinese(text):
            return text
            
        cache_key = f"{text[:200]}"
        if cache_key in self._cache:
            return self._cache[cache_key]
            
        try:
            result = ts.translate_text(
                text, 
                translator=self.translator_engine,
                from_language='en', 
                to_language='zh'
            )
            self._cache[cache_key] = result
            return result
        except Exception as e:
            return text
    
    def _is_mostly_chinese(self, text: str) -> bool:
        if not text:
            return False
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        return chinese_chars / len(text) > 0.4
    
    def translate_items(self, items: List[Dict], fields: List[str] = None, max_items: int = 10) -> List[Dict]:
        if not TRANSLATOR_AVAILABLE or not items:
            return items
            
        fields = fields or ['title', 'summary']
        items_to_translate = items[:max_items]
        
        print(f"æ­£åœ¨ç¿»è¯‘ {len(items_to_translate)} æ¡æ–°é—»...")
        for i, item in enumerate(items_to_translate, 1):
            print(f"  [{i}/{len(items_to_translate)}] {item.get('title', '')[:40]}...", end='\r')
            for field in fields:
                if field in item and item[field]:
                    text = item[field][:800] if field == 'summary' else item[field][:200]
                    item[field] = self.translate(text)
        print(f"\nç¿»è¯‘å®Œæˆï¼")
        return items


class NewsFetcher:
    """è·å–å’Œå¤„ç† AI æ–°é—»"""
    
    def __init__(self, sources: Optional[List[str]] = None, translator: Optional[NewsTranslator] = None):
        self.sources = sources or list(SOURCES.keys())
        self.news_items: List[Dict] = []
        self.translator = translator
        
    def fetch_rss(self, source_key: str) -> List[Dict]:
        """ä» RSS æºè·å–æ–°é—»"""
        if not FEEDPARSER_AVAILABLE:
            print("è­¦å‘Šï¼šæœªå®‰è£… feedparser")
            return []
            
        source = SOURCES.get(source_key)
        if not source:
            return []
            
        try:
            print(f"  æ­£åœ¨è·å–: {source['name']}...")
            feed = feedparser.parse(source["url"])
            items = []
            
            for entry in feed.entries[:30]:
                published = entry.get("published", entry.get("updated", "æœªçŸ¥"))
                
                item = {
                    "title": entry.get("title", "æ— æ ‡é¢˜"),
                    "link": entry.get("link", ""),
                    "summary": entry.get("summary", entry.get("description", ""))[:400],
                    "published": published,
                    "source": source["name"],
                    "category": source.get("category", "general"),
                    "language": source.get("language", "en")
                }
                
                # è‡ªåŠ¨è¯†åˆ«å…¬å¸å’Œæœºæ„
                full_text = f"{item['title']} {item['summary']}"
                item["companies"] = detect_companies(full_text)
                
                items.append(item)
                
            print(f"    âœ“ è·å–åˆ° {len(items)} æ¡")
            return items
        except Exception as e:
            print(f"    âœ— è·å–å¤±è´¥ï¼š{e}")
            return []
    
    def fetch_all(self, days: int = 1, strict_date_filter: bool = True) -> List[Dict]:
        """ä»æ‰€æœ‰æ¥æºè·å–æ–°é—»"""
        all_news = []
        
        for source_key in self.sources:
            if source_key in SOURCES:
                items = self.fetch_rss(source_key)
                all_news.extend(items)
        
        # æ—¥æœŸè¿‡æ»¤
        if strict_date_filter and days > 0:
            cutoff = datetime.now() - timedelta(days=days)
            cutoff = cutoff.replace(hour=0, minute=0, second=0, microsecond=0)
            
            filtered_news = []
            for item in all_news:
                pub_date = parse_date(item.get("published", ""))
                if pub_date:
                    if pub_date.tzinfo:
                        pub_date = pub_date.replace(tzinfo=None)
                    if pub_date >= cutoff:
                        item["_parsed_date"] = pub_date
                        filtered_news.append(item)
                else:
                    if days > 1:
                        filtered_news.append(item)
            
            all_news = filtered_news
            print(f"\næ—¥æœŸè¿‡æ»¤å: {len(all_news)} æ¡æ–°é—»ï¼ˆæœ€è¿‘ {days} å¤©ï¼‰")
        
        # æŒ‰æ—¥æœŸæ’åº
        all_news.sort(key=lambda x: x.get("_parsed_date", datetime.min), reverse=True)
        
        self.news_items = all_news
        return all_news
    
    def filter_by_category(self, categories: List[str]) -> List[Dict]:
        """æŒ‰åˆ†ç±»ç­›é€‰"""
        return [item for item in self.news_items 
                if item.get("category") in categories]
    
    def search(self, keyword: str) -> List[Dict]:
        """å…³é”®è¯æœç´¢"""
        keyword = keyword.lower()
        return [item for item in self.news_items
                if keyword in item.get("title", "").lower() 
                or keyword in item.get("summary", "").lower()]


class NewsFormatter:
    """æ ¼å¼åŒ–æ–°é—»è¾“å‡º"""
    
    def __init__(self, news_items: List[Dict]):
        self.news_items = news_items
        
    def to_markdown(self, format_type: str = "standard") -> str:
        if format_type == "newsletter":
            return self._to_newsletter_markdown()
        elif format_type == "summary":
            return self._to_summary_markdown()
        else:
            return self._to_standard_markdown()
    
    def _format_companies(self, companies: List[str]) -> str:
        """æ ¼å¼åŒ–å…¬å¸æ ‡ç­¾"""
        if not companies:
            return ""
        return " Â· ".join([f"ğŸ¢ {c}" for c in companies[:3]])
    
    def _to_newsletter_markdown(self, title: str = "æ¯æ—¥ AI ç®€æŠ¥", intro: str = "") -> str:
        """ç®€æ´ç¾è§‚çš„æ–°é—»é€šè®¯æ ¼å¼"""
        today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
        
        lines = [
            f"# {title}",
            "",
            f"ğŸ“… **{today}** | ğŸ¤– ç²¾é€‰ {len(self.news_items)} æ¡ AI åœˆé‡è¦åŠ¨æ€",
            "",
            "---",
            ""
        ]
        
        for i, item in enumerate(self.news_items, 1):
            # æ¥æºæ ‡ç­¾
            source_tag = f"ğŸ“° {item['source']}"
            
            # å…¬å¸æ ‡ç­¾
            companies = self._format_companies(item.get("companies", []))
            meta = f"{source_tag}" + (f" | {companies}" if companies else "")
            
            # æ‘˜è¦å¤„ç†
            summary = item['summary'][:200] + "..." if len(item['summary']) > 200 else item['summary']
            # ç§»é™¤ HTML æ ‡ç­¾
            summary = re.sub(r'<[^>]+>', '', summary)
            
            lines.extend([
                f"### {i}. {item['title']}",
                "",
                f"{summary}",
                "",
                f"*{meta}*",
                f"[â†’ é˜…è¯»åŸæ–‡]({item['link']})",
                "",
                "---",
                ""
            ])
        
        lines.extend([
            "",
            "ğŸ’¡ *æœ¬ç®€æŠ¥ç”± AI è‡ªåŠ¨ç”Ÿæˆ*",
            ""
        ])
        
        return "\n".join(lines)
    
    def _to_standard_markdown(self) -> str:
        """æ ‡å‡†åˆ†ç±»æ ¼å¼"""
        lines = [
            "# ğŸ¤– AI æ–°é—»æ—¥æŠ¥",
            "",
            f"ğŸ“… {datetime.now().strftime('%Y-%m-%d')} | å…± {len(self.news_items)} æ¡",
            ""
        ]
        
        # æŒ‰åˆ†ç±»åˆ†ç»„
        by_category: Dict[str, List[Dict]] = {}
        for item in self.news_items:
            cat = item.get("category", "general")
            by_category.setdefault(cat, []).append(item)
        
        category_names = {
            "releases": "ğŸš€ æ–°å‘å¸ƒ",
            "research": "ğŸ”¬ ç ”ç©¶åŠ¨æ€",
            "business": "ğŸ’° å•†ä¸šèµ„è®¯",
            "products": "ğŸ“± äº§å“æ›´æ–°",
            "community": "ğŸ’¬ ç¤¾åŒºåŠ¨æ€",
            "general": "ğŸ“° ç»¼åˆ"
        }
        
        for category, items in by_category.items():
            cn_name = category_names.get(category, category)
            lines.extend([f"## {cn_name}", ""])
            
            for item in items:
                companies = self._format_companies(item.get("companies", []))
                meta = f"ğŸ“° {item['source']}" + (f" | {companies}" if companies else "")
                
                lines.extend([
                    f"### {item['title']}",
                    "",
                    f"{item['summary'][:250]}...",
                    "",
                    f"*{meta}* | [é˜…è¯»åŸæ–‡]({item['link']})",
                    ""
                ])
                
        return "\n".join(lines)
    
    def _to_summary_markdown(self) -> str:
        """ç®€æ´æ‘˜è¦æ ¼å¼"""
        lines = [
            "# AI æ–°é—»æ‘˜è¦",
            "",
            f"*{datetime.now().strftime('%Y-%m-%d')} - å…± {len(self.news_items)} æ¡*",
            ""
        ]
        
        for item in self.news_items[:20]:
            companies = self._format_companies(item.get("companies", []))
            source_info = f"ğŸ“° {item['source']}"
            if companies:
                source_info += f" | {companies}"
            
            lines.append(f"â€¢ **{item['title']}** â€” *{source_info}*")
            
        return "\n".join(lines)
    
    def to_json(self) -> str:
        return json.dumps({
            "generated_at": datetime.now().isoformat(),
            "count": len(self.news_items),
            "items": self.news_items
        }, indent=2, ensure_ascii=False)
    
    def to_text(self) -> str:
        lines = [
            "AI æ¯æ—¥æ–°é—»",
            f"æ—¥æœŸï¼š{datetime.now().strftime('%Y-%m-%d')}",
            f"æ¡æ•°ï¼š{len(self.news_items)}",
            "=" * 50,
            ""
        ]
        
        for i, item in enumerate(self.news_items[:20], 1):
            companies = ", ".join(item.get("companies", [])) or "æœªçŸ¥"
            lines.extend([
                f"{i}. {item['title']}",
                f"   æ¥æºï¼š{item['source']} | å…¬å¸ï¼š{companies}",
                f"   é“¾æ¥ï¼š{item['link']}",
                ""
            ])
            
        return "\n".join(lines)


def parse_args():
    parser = argparse.ArgumentParser(description="AI æ¯æ—¥æ–°é—»")
    
    parser.add_argument("--date", choices=["today", "week", "month"], default="today")
    parser.add_argument("--days", type=int, default=1, help="å›æº¯å¤©æ•°")
    parser.add_argument("--categories", help="åˆ†ç±»ç­›é€‰")
    parser.add_argument("--search", help="å…³é”®è¯æœç´¢")
    parser.add_argument("--output", choices=["markdown", "json", "text"], default="markdown")
    parser.add_argument("--format", choices=["standard", "summary", "newsletter"], default="newsletter")
    parser.add_argument("--sources", help="æŒ‡å®šæ–°é—»æº")
    parser.add_argument("--save-to", help="ä¿å­˜è·¯å¾„")
    parser.add_argument("--max-items", type=int, default=15)
    parser.add_argument("--title", default="ğŸ¤– AI æ¯æ—¥ç®€æŠ¥")
    parser.add_argument("--intro", default="")
    parser.add_argument("--translate", action="store_true")
    parser.add_argument("--translate-fields", default="title,summary")
    parser.add_argument("--no-date-filter", action="store_true")
    parser.add_argument("--include-github", action="store_true", help="åŒ…å« GitHub æ•°æ®æº")
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # ç¡®å®šæ–°é—»æº
    if args.sources:
        sources = args.sources.split(",")
    else:
        # é»˜è®¤ä½¿ç”¨ä¸“ä¸šæ–°é—»ç½‘ç«™ï¼ˆä¸åŒ…å« GitHubï¼‰
        sources = [
            "marktechpost",
            "mit-tech-review",
            "venturebeat-ai",
            "synced-review",
            "jiqizhixin",
            "qbitai",
        ]
    
    # ç¡®å®šå¤©æ•°
    if args.days != 1:
        days = args.days
    else:
        days_map = {"today": 1, "week": 7, "month": 30}
        days = days_map.get(args.date, args.days)
    
    # åˆå§‹åŒ–ç¿»è¯‘å™¨
    translator = None
    if args.translate:
        if not TRANSLATOR_AVAILABLE:
            print("è­¦å‘Šï¼šæœªå®‰è£… translators åº“ï¼Œæ— æ³•ç¿»è¯‘")
        else:
            translator = NewsTranslator()
    
    # è·å–æ–°é—»
    print(f"æ­£åœ¨è·å– AI æ–°é—»ï¼ˆæœ€è¿‘ {days} å¤©ï¼‰...")
    print("=" * 50)
    
    fetcher = NewsFetcher(sources=sources, translator=translator)
    news = fetcher.fetch_all(days=days, strict_date_filter=not args.no_date_filter)
    
    # ç­›é€‰
    if args.categories:
        news = fetcher.filter_by_category(args.categories.split(","))
    if args.search:
        news = fetcher.search(args.search)
    
    news = news[:args.max_items]
    
    # ç¿»è¯‘
    if args.translate and translator and news:
        news = translator.translate_items(news, fields=args.translate_fields.split(","), max_items=args.max_items)
    
    print(f"\næœ€ç»ˆè¾“å‡º: {len(news)} æ¡æ–°é—»")
    
    # æ ¼å¼åŒ–
    formatter = NewsFormatter(news)
    
    if args.output == "json":
        output = formatter.to_json()
    elif args.output == "text":
        output = formatter.to_text()
    else:
        output = formatter.to_markdown(format_type=args.format)
    
    # ä¿å­˜æˆ–è¾“å‡º
    if args.save_to:
        os.makedirs(os.path.dirname(args.save_to) or ".", exist_ok=True)
        with open(args.save_to, "w", encoding="utf-8") as f:
            f.write(output)
        print(f"å·²ä¿å­˜è‡³ï¼š{args.save_to}")
    else:
        print(output)


if __name__ == "__main__":
    main()
